{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wJ6oYeTIg0s"
   },
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW7Zlev0Ig0w"
   },
   "source": [
    "# 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YgLRrXGIg0x"
   },
   "source": [
    "오늘의 과정을 마치신 것을 축하드립니다! 유익한 기술을 배우고 즐거운 시간 보내셨기를 바랍니다. 이제 습득한 기술을 테스트해볼 시간입니다. 이 평가에서는 신선한 과일과 썩은 과일을 인식할 수 있는 새 모델을 트레이닝하게 됩니다. 모델의 검증 정확도 `92%`에 도달해야 평가에 합격할 수 있지만, 가능하다면 더 높은 점수를 달성해 보시기 바랍니다. 이전 연습에서 배운 기술을 사용해야 합니다. 구체적으로는 전이 학습, 데이터 증강 및 파인튜닝을 조합해서 사용할 것을 권장합니다. 검증 데이터세트에 대해 최소 92%의 정확도에 도달하도록 모델을 트레이닝한 후에는 모델을 저장한 다음 정확도를 평가하십시오. 시작하겠습니다! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HBQXUZIIg0x"
   },
   "source": [
    "## 데이터세트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpTvlwXlIg0x"
   },
   "source": [
    "이 연습에서는 신선한 과일과 썩은 과일을 인식하도록 모델을 트레이닝하게 됩니다. 데이터세트는 [Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification)에서 가져오게 됩니다. Kaggle은 이 수업 이후에 프로젝트를 시작하는 데 관심이 있는 경우 유용하게 활용할 수 있는 웹사이트입니다. 데이터세트 구조는 `fruits` 폴더에 있습니다. 신선한 사과, 신선한 오렌지, 신선한 바나나, 썩은 사과, 썩은 오렌지, 썩은 바나나, 이렇게 여섯 가지 과일 범주가 있습니다. 즉, 성공적으로 분류를 수행하기 위해서는 모델에 6개의 뉴런으로 이루어진 출력 레이어가 필요합니다. 또한 범주가 3개 이상이므로 `categorical_crossentropy`로 모델을 컴파일해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqdhgcTFIg0y"
   },
   "source": [
    "<img src=\"./images/fruits.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zfto0u66Ig0y"
   },
   "source": [
    "## ImageNet 기본 모델 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqwGKQloIg0y"
   },
   "source": [
    "ImageNet으로 사전 트레이닝된 모델로 시작할 것을 권장합니다. 가중치가 올바른 모델을 로드하고 입력 모양을 설정하고 모델의 마지막 레이어를 제거하도록 선택합니다. 이미지에는 높이, 너비와 채널 수, 이렇게 세 개의 차원이 있습니다. 이러한 사진은 컬러 형식이므로 빨간색, 초록색, 파란색을 위한 3개의 채널이 있습니다. 입력 모양은 채워져 있습니다. 이를 변경할 수 없으며, 변경할 경우 평가에서 불합격하게 됩니다. 사전 트레이닝된 모델을 설정하기 위한 참조자료가 필요한 경우 전이 학습을 구현한 [노트북 05b](05b_presidential_doggy_door.ipynb)를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CdrwmLSQIg0y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "base_model = keras.applications.VGG16(\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False) #사전트레이닝 \n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybIUt6o0Ig0z"
   },
   "source": [
    "## 기본 모델 동결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3aPKkXqIg0z"
   },
   "source": [
    "다음으로는, [notebook 05b](05b_presidential_doggy_door.ipynb)에서 했던 것처럼 기본 모델을 동결시킬 것을 권장합니다. 이렇게 하는 이유는 ImageNet 데이터세트에서 학습된 모든 내용이 초기 트레이닝에서 손상되지 않도록 하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VKQ09BZRIg0z"
   },
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wp6Hbo_jIg0z"
   },
   "source": [
    "## 모델에 레이어 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NKuAcpnIg00"
   },
   "source": [
    "이제 레이어를 사전 트레이닝된 모델에 추가해야 합니다. [노트북 05b](05b_presidential_doggy_door.ipynb)를 가이드로 활용할 수 있습니다. 마지막 밀집 레이어에 각별히 주의하여 올바른 뉴런 수를 포함해 다양한 유형의 과일을 분류할 수 있도록 하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6D9e8rv1Ig00"
   },
   "outputs": [],
   "source": [
    "# Create inputs with correct shape\n",
    "inputs = keras.Input(shape = (224, 224, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "# Add pooling layer or flatten layer\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add final dense layer\n",
    "outputs = keras.layers.Dense(6)(x)\n",
    "\n",
    "# Combine inputs and outputs to create model\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mm2wsBBRIg00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 14,717,766\n",
      "Trainable params: 3,078\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-L6_vwQIg00"
   },
   "source": [
    "## 모델 컴파일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "By7jPt10Ig00"
   },
   "source": [
    "손실 및 지표 옵션으로 모델을 컴파일해야 합니다. 바이너리 분류 문제가 아니라 여러 다양한 범주에 대한 트레이닝을 진행 중임을 기억하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kYyWvSMGIg00"
   },
   "outputs": [],
   "source": [
    "model.compile(loss =keras.losses.SparseCategoricalCrossentropy(from_logits = True) , metrics = [keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "870wj7JTIg01"
   },
   "source": [
    "## 데이터 증강"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10tYeJ1nIg01"
   },
   "source": [
    "원하는 경우 데이터를 증강하여 데이터세트를 개선해 보십시오. 증강 예시는 [노트북 04a](04a_asl_augmentation.ipynb) 및 [노트북 05b](05b_presidential_doggy_door.ipynb)를 자유롭게 참조하십시오. [Keras ImageDataGenerator 클래스](https://keras.io/api/preprocessing/image/#imagedatagenerator-class)에 관한 문서도 있습니다. 이 단계는 선택 사항이지만 92% 정확도를 달성하는 데 도움이 될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T0N9e9abIg01"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        samplewise_center = True,\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOSEWqepIg01"
   },
   "source": [
    "## 데이터세트 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjAYyriYIg01"
   },
   "source": [
    "이제 트레이닝 및 검증 데이터세트를 로드해야 합니다. 이미지의 적절한 폴더와 적절한 `target_size`를 선택하십시오(생성한 모델의 높이 및 너비 입력과 일치해야 함). 참조 자료를 원한다면 [노트북 05b](05b_presidential_doggy_door.ipynb)를 확인하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Dmyu7m8oIg01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1183 images belonging to 6 classes.\n",
      "Found 329 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# load and iterate training dataset\n",
    "train_it = datagen.flow_from_directory('data/fruits/train/', \n",
    "                                       target_size=(224, 224), \n",
    "                                       color_mode='rgb', \n",
    "                                       class_mode=\"binary\"                                      )\n",
    "# load and iterate validation dataset\n",
    "valid_it = datagen.flow_from_directory('data/fruits/valid/', \n",
    "                                      target_size=(224, 224), \n",
    "                                      color_mode='rgb', \n",
    "                                      class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 224, 224, 3), (32,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = train_it.next()\n",
    "x.shape, y.shape # 6 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 4., 0., 4., 3., 2., 2., 3., 4., 0., 1., 0., 4., 4., 5., 4., 5.,\n",
       "       3., 1., 0., 3., 2., 2., 3., 3., 2., 1., 4., 3., 4., 4., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvlWUCfwIg02"
   },
   "source": [
    "## 모델 트레이닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AleJbHXsIg02"
   },
   "source": [
    "이제 모델을 트레이닝할 시간입니다! `train` 및 `valid` 반복자(iterator)를 `fit` 함수로 전달하고 원하는 에포크 수를 설정하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "m2pwV968Ig02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "37/36 [==============================] - 29s 773ms/step - loss: 2.8947 - sparse_categorical_accuracy: 0.3863 - val_loss: 1.2379 - val_sparse_categorical_accuracy: 0.6413\n",
      "Epoch 2/20\n",
      "37/36 [==============================] - 19s 519ms/step - loss: 0.8463 - sparse_categorical_accuracy: 0.7396 - val_loss: 0.6387 - val_sparse_categorical_accuracy: 0.8055\n",
      "Epoch 3/20\n",
      "37/36 [==============================] - 19s 515ms/step - loss: 0.4065 - sparse_categorical_accuracy: 0.8639 - val_loss: 0.3913 - val_sparse_categorical_accuracy: 0.8754\n",
      "Epoch 4/20\n",
      "37/36 [==============================] - 19s 513ms/step - loss: 0.2769 - sparse_categorical_accuracy: 0.8943 - val_loss: 0.3871 - val_sparse_categorical_accuracy: 0.8784\n",
      "Epoch 5/20\n",
      "37/36 [==============================] - 19s 514ms/step - loss: 0.1775 - sparse_categorical_accuracy: 0.9324 - val_loss: 0.3324 - val_sparse_categorical_accuracy: 0.9119\n",
      "Epoch 6/20\n",
      "37/36 [==============================] - 19s 516ms/step - loss: 0.1540 - sparse_categorical_accuracy: 0.9442 - val_loss: 0.2374 - val_sparse_categorical_accuracy: 0.9301\n",
      "Epoch 7/20\n",
      "37/36 [==============================] - 19s 513ms/step - loss: 0.1113 - sparse_categorical_accuracy: 0.9620 - val_loss: 0.2648 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 8/20\n",
      "37/36 [==============================] - 19s 515ms/step - loss: 0.0932 - sparse_categorical_accuracy: 0.9662 - val_loss: 0.1950 - val_sparse_categorical_accuracy: 0.9362\n",
      "Epoch 9/20\n",
      "37/36 [==============================] - 19s 515ms/step - loss: 0.0889 - sparse_categorical_accuracy: 0.9704 - val_loss: 0.2815 - val_sparse_categorical_accuracy: 0.9240\n",
      "Epoch 10/20\n",
      "37/36 [==============================] - 19s 521ms/step - loss: 0.0612 - sparse_categorical_accuracy: 0.9746 - val_loss: 0.1254 - val_sparse_categorical_accuracy: 0.9635\n",
      "Epoch 11/20\n",
      "37/36 [==============================] - 19s 509ms/step - loss: 0.0561 - sparse_categorical_accuracy: 0.9831 - val_loss: 0.1794 - val_sparse_categorical_accuracy: 0.9483\n",
      "Epoch 12/20\n",
      "37/36 [==============================] - 19s 515ms/step - loss: 0.0541 - sparse_categorical_accuracy: 0.9839 - val_loss: 0.1485 - val_sparse_categorical_accuracy: 0.9453\n",
      "Epoch 13/20\n",
      "37/36 [==============================] - 19s 512ms/step - loss: 0.0465 - sparse_categorical_accuracy: 0.9873 - val_loss: 0.2350 - val_sparse_categorical_accuracy: 0.9392\n",
      "Epoch 14/20\n",
      "37/36 [==============================] - 19s 513ms/step - loss: 0.0407 - sparse_categorical_accuracy: 0.9873 - val_loss: 0.1502 - val_sparse_categorical_accuracy: 0.9422\n",
      "Epoch 15/20\n",
      "37/36 [==============================] - 19s 513ms/step - loss: 0.0308 - sparse_categorical_accuracy: 0.9907 - val_loss: 0.1297 - val_sparse_categorical_accuracy: 0.9635\n",
      "Epoch 16/20\n",
      "37/36 [==============================] - 19s 513ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9924 - val_loss: 0.0822 - val_sparse_categorical_accuracy: 0.9696\n",
      "Epoch 17/20\n",
      "37/36 [==============================] - 19s 515ms/step - loss: 0.0251 - sparse_categorical_accuracy: 0.9907 - val_loss: 0.1883 - val_sparse_categorical_accuracy: 0.9605\n",
      "Epoch 18/20\n",
      "37/36 [==============================] - 19s 515ms/step - loss: 0.0228 - sparse_categorical_accuracy: 0.9941 - val_loss: 0.1221 - val_sparse_categorical_accuracy: 0.9605\n",
      "Epoch 19/20\n",
      "37/36 [==============================] - 19s 514ms/step - loss: 0.0256 - sparse_categorical_accuracy: 0.9907 - val_loss: 0.1324 - val_sparse_categorical_accuracy: 0.9757\n",
      "Epoch 20/20\n",
      "37/36 [==============================] - 19s 515ms/step - loss: 0.0260 - sparse_categorical_accuracy: 0.9907 - val_loss: 0.1222 - val_sparse_categorical_accuracy: 0.9605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcd34049550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lEYo0WaIg02"
   },
   "source": [
    "## 파인튜닝을 위해 모델 동결 해제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRTTMt0YIg02"
   },
   "source": [
    "이미 92%의 검증 정확도에 도달한 경우 이 다음 단계는 선택 사항입니다. 도달하지 않은 경우에는 매우 낮은 학습률로 모델을 파인튜닝할 것을 권장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BBoy9p_TIg02"
   },
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Compile the model with a low learning rate\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate = .00001),\n",
    "              loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = [keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dE_i0hcIg03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/36 [==============================] - 34s 926ms/step - loss: 0.0670 - sparse_categorical_accuracy: 0.9856 - val_loss: 0.1554 - val_sparse_categorical_accuracy: 0.9666\n",
      "Epoch 2/10\n",
      "37/36 [==============================] - 23s 630ms/step - loss: 0.0258 - sparse_categorical_accuracy: 0.9915 - val_loss: 0.1172 - val_sparse_categorical_accuracy: 0.9818\n",
      "Epoch 3/10\n",
      "37/36 [==============================] - 22s 586ms/step - loss: 0.0263 - sparse_categorical_accuracy: 0.9941 - val_loss: 0.1637 - val_sparse_categorical_accuracy: 0.9726\n",
      "Epoch 4/10\n",
      "37/36 [==============================] - 21s 570ms/step - loss: 0.0352 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.0784 - val_sparse_categorical_accuracy: 0.9878\n",
      "Epoch 5/10\n",
      "37/36 [==============================] - 21s 569ms/step - loss: 0.0134 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.3956 - val_sparse_categorical_accuracy: 0.9210\n",
      "Epoch 6/10\n",
      "37/36 [==============================] - 21s 567ms/step - loss: 0.0090 - sparse_categorical_accuracy: 0.9966 - val_loss: 0.0542 - val_sparse_categorical_accuracy: 0.9878\n",
      "Epoch 7/10\n",
      "13/36 [=========>....................] - ETA: 9s - loss: 0.0011 - sparse_categorical_accuracy: 1.0000 "
     ]
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9XFIsMIIg03"
   },
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGa_jUEgIg03"
   },
   "source": [
    "이제 모델의 검증 정확도 92% 이상에 도달했다면 좋겠지만 그렇지 못한 경우에는 돌아가서 트레이닝의 에포크를 더 실행하거나 데이터 증강을 조정해야 할 수 있습니다. \n",
    "\n",
    "검증 정확도가 만족스러운 경우에는 다음 셀을 실행하여 모델을 평가하십시오. 평가 함수는 튜플을 반환하며, 여기서 첫 번째 값은 손실, 두 번째 값은 정확도입니다. 합격하려면 모델의 정확도 값이 `92% or higher`이어야 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VzfCZ4lEIg03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [================================] - 4s 337ms/step - loss: 0.1222 - sparse_categorical_accuracy: 0.9696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12217158079147339, 0.9696048498153687]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_it, steps=valid_it.samples/valid_it.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYqb9kqAIg03"
   },
   "source": [
    "## 평가 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgGJsI_zIg03"
   },
   "source": [
    "모델을 평가하려면 다음 두 개의 셀을 실행하십시오.\n",
    "\n",
    "**참고:** `run_assessment`는 모델이 `model`로 명명되었고 검증 데이터 반복자가 `valid_it`으로 명명되었음을 가정합니다. 어떤 이유로든 이러한 변수 이름을 수정한 경우에는 `run_assessment`로 전달된 인수의 이름을 업데이트하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cZBby8oFIg03"
   },
   "outputs": [],
   "source": [
    "from run_assessment import run_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bAypQ-nmIg04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model 5 times to obtain average accuracy...\n",
      "\n",
      "11/10 [================================] - 4s 370ms/step - loss: 0.1581 - sparse_categorical_accuracy: 0.9635\n",
      "11/10 [================================] - 4s 329ms/step - loss: 0.1556 - sparse_categorical_accuracy: 0.9605\n",
      "11/10 [================================] - 4s 341ms/step - loss: 0.2035 - sparse_categorical_accuracy: 0.9605\n",
      "11/10 [================================] - 4s 342ms/step - loss: 0.1508 - sparse_categorical_accuracy: 0.9574\n",
      "11/10 [================================] - 4s 340ms/step - loss: 0.1205 - sparse_categorical_accuracy: 0.9696\n",
      "\n",
      "Accuracy required to pass the assessment is 0.92 or greater.\n",
      "Your average accuracy is 0.9623.\n",
      "\n",
      "Congratulations! You passed the assessment!\n",
      "See instructions below to generate a certificate.\n"
     ]
    }
   ],
   "source": [
    "run_assessment(model, valid_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7OG25ZOIg04"
   },
   "source": [
    "## 인증서 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQ2PbtXjIg04"
   },
   "source": [
    "평가에 합격한 경우 아래에 표시된 과정 페이지로 돌아가 \"평가 작업\" 버튼을 클릭하면 과정 인증서가 생성됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM9uO2EWIg04"
   },
   "source": [
    "<img src=\"./images/assess_task.png\" style=\"width: 800px;\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task1_task_07_assessment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
