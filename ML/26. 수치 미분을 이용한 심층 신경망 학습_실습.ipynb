{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"26. 수치 미분을 이용한 심층 신경망 학습_실습.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Z05lTWb0YGFx"},"source":["## 수치 미분을 이용한 심층 신경망 학습"]},{"cell_type":"code","metadata":{"id":"3cQiN1pNYGF4","executionInfo":{"status":"ok","timestamp":1634211447326,"user_tz":-540,"elapsed":6,"user":{"displayName":"Harim Choi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11352501161191428326"}}},"source":["import time\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sXfo91iRYGF6"},"source":["## 유틸리티 함수"]},{"cell_type":"code","metadata":{"id":"lZtBsmNCYGF7","executionInfo":{"status":"ok","timestamp":1634211447327,"user_tz":-540,"elapsed":5,"user":{"displayName":"Harim Choi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11352501161191428326"}}},"source":["epsilon = 0.0001\n","\n","def _t(x):\n","    return np.transpose(x)\n","\n","def _m(A, B):\n","    return np.matmul(A, B)\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def mean_squared_error(h, y):\n","    return 1 / 2 * np.mean(np.square(h - y))"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sb8yPxzjYGF8"},"source":["## 뉴런 구현"]},{"cell_type":"code","metadata":{"id":"KUqzDsx_YGF8","executionInfo":{"status":"ok","timestamp":1634211639792,"user_tz":-540,"elapsed":246,"user":{"displayName":"Harim Choi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11352501161191428326"}}},"source":["class Neuron:\n","    def __init__(self, W, b, a):\n","      self.W = W\n","      self.b = b\n","      self.a = a #activation , sigmoid\n","\n","      #gradient\n","      self.dW = np.zeros_like(self.W)\n","      self.db = np.zeros_like(self.b)\n","\n","    def __call__(self, x): #  a(W * x + b)\n","      return self.a(_m(_t(self.W), x) + self.b)\n","        "],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IWFhv-9UYGF9"},"source":["## 심층신경망 구현"]},{"cell_type":"code","metadata":{"id":"xbecMjpKYGF-","executionInfo":{"status":"ok","timestamp":1634213091658,"user_tz":-540,"elapsed":309,"user":{"displayName":"Harim Choi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11352501161191428326"}}},"source":["class DNN:\n","    def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=sigmoid):\n","        def init_var(i, o):\n","            return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,)) # W, b, 초기화 ; W (i, o) b (o, )\n","\n","        self.sequence = list()\n","        # First hidden layer \n","        W, b = init_var(num_input, num_neuron)\n","        self.sequence.append(Neuron(W, b, activation))\n","        \n","        # Hidden layers\n","        for _ in range(hidden_depth-1):\n","          W, b = init_var(num_neuron, num_neuron)\n","          self.sequence.append(Neuron(W, b, activation))\n","\n","        # Output layer\n","        W, b = init_var(num_neuron, num_output)\n","        self.sequence.append(Neuron(W, b, activation))\n","\n","    def __call__(self, x):\n","        for layer in self.sequence:\n","            x = layer(x)\n","        return x\n","\n","    def calc_gradient(self, x, y, loss_func): #dW, dh 구하기, vector gradient \n","        def get_new_sequence(layer_index, new_neuron):\n","          new_sequence = list()\n","          for i, layer in enumerate(self.sequence):\n","            if i == layer_index:\n","              new_sequence.append(new_neuron)\n","            else:\n","              new_sequence.append(layer)\n","          return new_sequence\n","        \n","        def eval_sequence(x, sequence):\n","          for layer in sequence:\n","            x = layer(x)\n","          return x\n","\n","        loss = loss_func(self(x), y)\n","        for layer_id, layer in enumerate(self.sequence): # layer마다\n","          # dW\n","          for w_i, w in enumerate(layer.W): # W col 마다\n","            for w_j, ww in enumerate(w): #col의 row\n","              W = np.copy(layer.W)\n","              W[w_i][w_j] = ww + epsilon  # f(W + e) - f(W) / epsilon \n","\n","              new_neuron = Neuron(W, layer.b, layer.a)\n","              new_seq = get_new_sequence(layer_id, new_neuron)\n","              h = eval_sequence(x, new_seq)\n","\n","              # L(W+ e) - L(W) / epsilon\n","              num_grad = (loss_func(h, y) - loss) / epsilon\n","              layer.dW[w_i][w_j] = num_grad\n","\n","          #db\n","          for b_i, bb in enumerate(layer.b): #(o,) #layer마다 , b_i마다\n","              b = np.copy(layer.b)\n","              b[b_i] = bb + epsilon # f(b + e)\n","\n","              new_neuron = Neuron(layer.W, b, layer.a)\n","              new_seq = get_new_sequence(layer_id, new_neuron)\n","              h = eval_sequence(x, new_seq) \n","\n","              num_grad = (loss_func(h, y) - loss) / epsilon #L(b + e) - L / epsilon\n","              layer.db[b_i] = num_grad\n","        \n","        return loss\n","\n","\n","\n"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ddnY_JERYGF_"},"source":["## 경사하강 학습법"]},{"cell_type":"code","metadata":{"id":"XhG5nuIdYGGA","executionInfo":{"status":"ok","timestamp":1634213093603,"user_tz":-540,"elapsed":308,"user":{"displayName":"Harim Choi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11352501161191428326"}}},"source":["def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n","    loss = network.calc_gradient(x, y, loss_obj)\n","    for layer in network.sequence:\n","        layer.W += -alpha * layer.dW\n","        layer.b += -alpha * layer.db\n","    return loss"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YU0D-s7BYGGB"},"source":["## 동작 테스트"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rJ6b51IhYGGC","outputId":"5b2382da-b55b-4cfa-af59-8bc023f88aeb"},"source":["x = np.random.normal(0.0, 1.0, (10,))\n","y = np.random.normal(0.0, 1.0, (2,))\n","\n","dnn = DNN(hidden_depth=5, num_neuron=32, num_input=10, num_output=2, activation=sigmoid)\n","\n","t = time.time()\n","for epoch in range(100):\n","    loss = gradient_descent(dnn, x, y, mean_squared_error, 0.01)\n","    print('Epoch {}: Test loss {}'.format(epoch, loss))\n","print('{} seconds elapsed.'.format(time.time() - t))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: Test loss 0.49067469887836956\n","Epoch 1: Test loss 0.4879254606874828\n","Epoch 2: Test loss 0.48519367996902363\n","Epoch 3: Test loss 0.48247966038542345\n","Epoch 4: Test loss 0.4797836909975817\n","Epoch 5: Test loss 0.4771060462533734\n","Epoch 6: Test loss 0.4744469860034807\n","Epoch 7: Test loss 0.4718067555433753\n","Epoch 8: Test loss 0.46918558568115953\n","Epoch 9: Test loss 0.4665836928294646\n","Epoch 10: Test loss 0.46400127912082956\n","Epoch 11: Test loss 0.4614385325452896\n","Epoch 12: Test loss 0.45889562710860293\n","Epoch 13: Test loss 0.45637272301066334\n","Epoch 14: Test loss 0.4538699668420847\n","Epoch 15: Test loss 0.4513874917984585\n","Epoch 16: Test loss 0.4489254179102646\n","Epoch 17: Test loss 0.4464838522879468\n","Epoch 18: Test loss 0.444062889380162\n","Epoch 19: Test loss 0.44166261124476985\n","Epoch 20: Test loss 0.4392830878306596\n","Epoch 21: Test loss 0.43692437726975675\n","Epoch 22: Test loss 0.43458652617743637\n","Epoch 23: Test loss 0.4322695699607737\n","Epoch 24: Test loss 0.4299735331334792\n","Epoch 25: Test loss 0.42769842963594507\n","Epoch 26: Test loss 0.42544426315993306\n","Epoch 27: Test loss 0.42321102747663997\n","Epoch 28: Test loss 0.4209987067672598\n","Epoch 29: Test loss 0.4188072759551456\n","Epoch 30: Test loss 0.4166367010386079\n","Epoch 31: Test loss 0.4144869394238685\n","Epoch 32: Test loss 0.4123579402568789\n","Epoch 33: Test loss 0.41024964475367864\n","Epoch 34: Test loss 0.40816198652881414\n","Epoch 35: Test loss 0.40609489192044745\n","Epoch 36: Test loss 0.40404828031232143\n","Epoch 37: Test loss 0.40202206445167343\n","Epoch 38: Test loss 0.40001615076279606\n","Epoch 39: Test loss 0.3980304396556399\n","Epoch 40: Test loss 0.39606482582899144\n","Epoch 41: Test loss 0.3941191985683081\n","Epoch 42: Test loss 0.3921934420373021\n","Epoch 43: Test loss 0.39028743556337847\n","Epoch 44: Test loss 0.3884010539165883\n","Epoch 45: Test loss 0.38653416758169673\n","Epoch 46: Test loss 0.3846866430236161\n","Epoch 47: Test loss 0.3828583429455882\n","Epoch 48: Test loss 0.38104912654018513\n","Epoch 49: Test loss 0.37925884973302115\n","Epoch 50: Test loss 0.377487365419505\n","Epoch 51: Test loss 0.37573452369349086\n","Epoch 52: Test loss 0.3740001720693107\n","Epoch 53: Test loss 0.3722841556958381\n","Epoch 54: Test loss 0.37058631756345706\n","Epoch 55: Test loss 0.3689064987035391\n","Epoch 56: Test loss 0.3672445383808169\n","Epoch 57: Test loss 0.36560027427840214\n","Epoch 58: Test loss 0.36397354267564175\n","Epoch 59: Test loss 0.3623641786191149\n","Epoch 60: Test loss 0.36077201608659093\n","Epoch 61: Test loss 0.3591968881439958\n","Epoch 62: Test loss 0.35763862709616334\n","Epoch 63: Test loss 0.3560970646305141\n","Epoch 64: Test loss 0.354572031954708\n","Epoch 65: Test loss 0.35306335992773674\n","Epoch 66: Test loss 0.35157087918515795\n","Epoch 67: Test loss 0.35009442025813\n","Epoch 68: Test loss 0.34863381368691704\n","Epoch 69: Test loss 0.3471888901284185\n","Epoch 70: Test loss 0.34575948045852933\n","Epoch 71: Test loss 0.34434541586893697\n","Epoch 72: Test loss 0.3429465279588903\n","Epoch 73: Test loss 0.3415626488219766\n","Epoch 74: Test loss 0.3401936111279358\n","Epoch 75: Test loss 0.33883924820004646\n","Epoch 76: Test loss 0.33749939408776175\n","Epoch 77: Test loss 0.33617388363512507\n","Epoch 78: Test loss 0.33486255254494846\n","Epoch 79: Test loss 0.33356523743901245\n","Epoch 80: Test loss 0.3322817759143043\n","Epoch 81: Test loss 0.3310120065955182\n","Epoch 82: Test loss 0.32975576918415245\n","Epoch 83: Test loss 0.3285129045038871\n"]}]},{"cell_type":"code","metadata":{"id":"YPMT-YdvrlSE"},"source":[""],"execution_count":null,"outputs":[]}]}